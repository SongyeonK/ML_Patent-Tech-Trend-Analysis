{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "!pip install pyLDAvis\n",
        "!pip install wordcloud\n",
        "!pip install gensim\n",
        "!pip install pickles"
      ],
      "metadata": {
        "id": "IGGHbpLMDP69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw9BKUzTBtd4"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV 파일 읽어오기\n",
        "data = pd.read_csv('final.csv', encoding='UTF-8')\n",
        "patents = data[['astrtCont', 'inventionTitle']].values.tolist()\n",
        "\n",
        "# patents 리스트의 길이 확인\n",
        "num_patents = len(patents)\n",
        "\n",
        "print(f\"특허 데이터의 개수: {num_patents}개\")"
      ],
      "metadata": {
        "id": "svZ3iAR2fgdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patents = []\n",
        "with open('final.csv', 'r', encoding='UTF-8') as file:  # CSV 파일 경로와 인코딩을 지정해주세요.\n",
        "    reader = csv.reader(file)\n",
        "    next(reader)  # 헤더 행 건너뛰기\n",
        "    for row in reader:\n",
        "        title = row[0]\n",
        "        abstract = row[1]\n",
        "        patents.append((title, abstract))"
      ],
      "metadata": {
        "id": "jzlJMC7-BzA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'[^가-힣\\s]', '', text)  # 한글과 공백을 제외한 모든 문자 제거\n",
        "    return text\n",
        "\n",
        "cleaned_patents = []\n",
        "for title, abstract in patents:\n",
        "    cleaned_title = clean_text(title)\n",
        "    cleaned_abstract = clean_text(abstract)\n",
        "    cleaned_patents.append((cleaned_title, cleaned_abstract))\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_patents = []\n",
        "for title, abstract in cleaned_patents:\n",
        "    tokens = [token for token, pos in okt.pos(title + ' ' + abstract) if pos.startswith(('N', 'V', 'A'))]\n",
        "    tokenized_patents.append(tokens)"
      ],
      "metadata": {
        "id": "wAefqjXjBzDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "9dd1BAcpkKFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_patents 파일로 저장\n",
        "tokenized_patents_file = '메모리_tokenized_patents.pkl'\n",
        "with open(tokenized_patents_file, 'wb') as file:\n",
        "    pickle.dump(tokenized_patents, file)\n",
        "print(\"tokenized_patents 파일을 저장했습니다.\")"
      ],
      "metadata": {
        "id": "rdnC11vLQbwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 tokenized_patents 파일 불러오기\n",
        "with open(tokenized_patents_file, 'rb') as file:\n",
        "    tokenized_patents = pickle.load(file)\n",
        "print(\"저장된 tokenized_patents 파일을 불러왔습니다.\")"
      ],
      "metadata": {
        "id": "7m9NMHPPYFHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 저장된 tokenized_patents 파일 불러오기\n",
        "# with open('tokenized_patents.pkl', 'rb') as file:\n",
        "#     tokenized_patents = pickle.load(file)\n",
        "# print(\"저장된 tokenized_patents 파일을 불러왔습니다.\")"
      ],
      "metadata": {
        "id": "8lEF48Ipq7yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 전체 코퍼스에서 단어 빈도수 계산\n",
        "word_counts = defaultdict(int)\n",
        "for tokens in tokenized_patents:\n",
        "    for token in tokens:\n",
        "        word_counts[token] += 1\n",
        "\n",
        "# 상위 N개 고빈도 단어 출력\n",
        "top_n = 100\n",
        "frequent_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "frequent_words = [word for word, count in frequent_words]\n",
        "print(frequent_words)"
      ],
      "metadata": {
        "id": "nq0az6E_QFDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 고빈도 단어 결과 참고하여 불용어 추가 선정\n",
        "stopwords = ['본', '발명', '은', '는', '이', '가', '의', '에', '을', '를', '것', '에서', '수', '등', '된', '및',\n",
        "             '종', '와', '한', '하기', '위한', '경우', '있는', '있다', '대한', '하다', '되다', '하는',\n",
        "             '상기', '제', '하여', '포함', '단계', '본', '것', '한다', '할', '생', '하나', '된다', '위', '관', '복수', '부', '또는', '적어도', '로부터',\n",
        "             '될', '중', '통해', '예', '되어', '이상', '다',  '그', '함', '일', '층', '상', '출', '검', '점', '의해', '되고', '성하는', '사용자', '트']"
      ],
      "metadata": {
        "id": "JilwsbYv3U96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 도메인 특화된 불용어 사전 추가\n",
        "domain_stopwords = ['특허', '기술', '장치', '방법', '시스템', '구성', '이용', '관련', '기반', '위해', '순서도', '도면', '예시', '실시', '다양', '적용', '대해', '되는',\n",
        "                    '데이터', '값', '하우', '징', '획득', '결과', '각각', '제공', '정보']\n",
        "\n",
        "filtered_patents = []\n",
        "for tokens in tokenized_patents:\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    filtered_patents.append(filtered_tokens)\n",
        "\n",
        "corpus = [' '.join(tokens) for tokens in filtered_patents]\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(corpus)\n",
        "\n",
        "dictionary = corpora.Dictionary(filtered_patents)\n",
        "corpus = [dictionary.doc2bow(text) for text in filtered_patents]"
      ],
      "metadata": {
        "id": "a2uxXg7yBzHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA 토픽 모델링 (토픽 수 범위 지정)\n",
        "topic_range = range(9, 15)  # 토픽 수 범위 (5부터 20까지)\n",
        "coherence_scores = []\n",
        "\n",
        "for num_topics in topic_range:\n",
        "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "\n",
        "    # 코히어런스 스코어 계산\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=filtered_patents, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "    print(f\"Number of Topics: {num_topics}, Coherence Score: {coherence_score:.4f}\")"
      ],
      "metadata": {
        "id": "Rqb1nRC1BzJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 토픽 수 선택\n",
        "best_topic_num = topic_range[coherence_scores.index(max(coherence_scores))]\n",
        "print(f\"Best Number of Topics: {best_topic_num}\")\n"
      ],
      "metadata": {
        "id": "dynPR-trBzME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 토픽 수로 LDA 모델 학습\n",
        "lda_model = LdaModel(corpus, num_topics=best_topic_num, id2word=dictionary)"
      ],
      "metadata": {
        "id": "XPAjvdFMBzOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 5  # 상위 5개 단어 선정\n",
        "topic_top_words = []\n",
        "for i in range(lda_model.num_topics):\n",
        "    topic_words = lda_model.get_topic_terms(i, topn=top_n)\n",
        "    top_words = [dictionary[word_id] for word_id, _ in topic_words]\n",
        "    topic_top_words.append(top_words)"
      ],
      "metadata": {
        "id": "PztslcsmmIqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(topic_top_words)"
      ],
      "metadata": {
        "id": "WWZOGHWyq9Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토픽 출력\n",
        "num_topics = best_topic_num  # 출력할 토픽의 수\n",
        "num_words = 10  # 각 토픽에서 출력할 단어의 수\n",
        "\n",
        "for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=num_words):\n",
        "    print(f\"Topic {idx+1}: {topic}\")"
      ],
      "metadata": {
        "id": "Jp-QICM9BzRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pyLDAvis 준비\n",
        "vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis_data)\n",
        "\n",
        "def get_word(word_id):\n",
        "    return dictionary.get(word_id)\n",
        "\n",
        "def get_topic_words(topic_id, top_n=10):\n",
        "    topic_words = lda_model.get_topic_terms(topic_id, topn=top_n)\n",
        "    return [get_word(word_id) for word_id, prob in topic_words]\n"
      ],
      "metadata": {
        "id": "pJlQRElJBzTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "plt.rcParams['axes.unicode_minus'] =False"
      ],
      "metadata": {
        "id": "Y8veQQf9kbuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 토픽에 대해 상위 단어들을 리스트로 출력\n",
        "for topic_id in range(lda_model.num_topics):\n",
        "    print(f\"Topic {topic_id+1}:\")\n",
        "    topic_words = get_topic_words(topic_id)\n",
        "    print(\", \".join(topic_words))\n",
        "    print()\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "for topic_id in range(lda_model.num_topics):\n",
        "    topic_words = get_topic_words(topic_id)\n",
        "\n",
        "    if not topic_words:\n",
        "        print(f\"Skipping Topic {topic_id} (no words assigned)\")\n",
        "        continue\n",
        "\n",
        "    wordcloud = WordCloud(width=400, height=300, background_color='white', font_path='/content/NanumGothic.ttf').generate(' '.join(topic_words))\n",
        "\n",
        "    plt.subplot(lda_model.num_topics // 3 + 1, 3, topic_id + 1)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Topic {topic_id+1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewuzOCrQBzV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서-토픽 분포 구하기\n",
        "doc_topic_dist = lda_model.get_document_topics(corpus)\n"
      ],
      "metadata": {
        "id": "SmwF1QAkBzYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토픽별 문서 수 계산\n",
        "topic_doc_counts = [0] * lda_model.num_topics\n",
        "for doc_topics in doc_topic_dist:\n",
        "    for topic_id, prob in doc_topics:\n",
        "        topic_doc_counts[topic_id] += 1"
      ],
      "metadata": {
        "id": "NEHF5tvhD5Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화\n",
        "topics = range(1, lda_model.num_topics+1)\n",
        "doc_counts = topic_doc_counts\n",
        "\n",
        "colors = ['#4CAF50', '#FF9800', '#673AB7', '#E91E63', '#FFC107', '#03A9F4', '#795548', '#9C27B0']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(topics, doc_counts, color=colors)\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.gca().annotate('{}'.format(int(height)),\n",
        "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                       xytext=(0, 3),\n",
        "                       textcoords=\"offset points\",\n",
        "                       ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel('Topic', fontsize=12)\n",
        "plt.ylabel('Document Count', fontsize=12)\n",
        "plt.title('Topic Distribution', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cPAiLdK2D5bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 연도별 토픽 분포 변화 시각화\n",
        "year_topic_dist = {}\n",
        "\n",
        "for i, (title, abstract) in enumerate(patents):\n",
        "    year = title[:4]  # 특허 제목에서 연도 추출 (예: 특허 제목 형식이 \"2021년 특허명\"인 경우)\n",
        "    if year not in year_topic_dist:\n",
        "        year_topic_dist[year] = [0] * best_topic_num\n",
        "\n",
        "    doc_topics = lda_model.get_document_topics(corpus[i])\n",
        "    for topic_id, prob in doc_topics:\n",
        "        year_topic_dist[year][topic_id] += prob"
      ],
      "metadata": {
        "id": "gIux67aZD5ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 연도별 토픽 분포 막대 그래프\n",
        "years = list(year_topic_dist.keys())\n",
        "topic_proportions = list(year_topic_dist.values())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "x = range(len(years))\n",
        "for i in range(best_topic_num):\n",
        "    topic_prop = [prop[i] for prop in topic_proportions]\n",
        "    ax.bar(x, topic_prop, width=0.8, label=f\"Topic {i+1}\")\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(years, rotation=45, ha='right')\n",
        "ax.set_xlabel(\"Year\", fontsize=12)\n",
        "ax.set_ylabel(\"Proportion\", fontsize=12)\n",
        "ax.set_title(\"Topic Distribution by Year\", fontsize=16)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cU5x32e-D-oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# 연도별 토픽 분포 데이터 준비\n",
        "years = sorted(list(year_topic_dist.keys()))\n",
        "num_topics = len(year_topic_dist[years[0]])\n",
        "\n",
        "# 색상 맵을 사용하여 colors 리스트 생성\n",
        "cmap = plt.cm.get_cmap('tab20')\n",
        "colors = [cmap(i) for i in range(num_topics)]\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "for i in range(num_topics):\n",
        "    topic_proportions = [year_topic_dist[year][i] for year in years]\n",
        "    plt.plot(years, topic_proportions, marker='o', linestyle='-', color=colors[i], label=f'Topic {i+1}')\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Proportion')\n",
        "plt.title('Topic Distribution by Year')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5c2KkLrN4dTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# 연도별 토픽 분포 데이터 준비\n",
        "years = sorted(list(year_topic_dist.keys()))\n",
        "num_topics = len(year_topic_dist[years[0]])\n",
        "\n",
        "# 연도별 토픽 비중 계산\n",
        "topic_proportions = {}\n",
        "for year in years:\n",
        "    topic_proportions[year] = [topic_count / sum(year_topic_dist[year]) for topic_count in year_topic_dist[year]]\n",
        "\n",
        "# 색상을 동적으로 생성\n",
        "colors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(num_topics)]\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "bar_width = 0.75\n",
        "x = range(len(years))\n",
        "\n",
        "bottom = [0] * len(years)\n",
        "for i in range(num_topics):\n",
        "    topic_props = [topic_proportions[year][i] for year in years]\n",
        "    plt.bar(x, topic_props, width=bar_width, bottom=bottom, color=colors[i], label=f'Topic {i+1}')\n",
        "    bottom = [b + p for b, p in zip(bottom, topic_props)]\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Proportion')\n",
        "plt.title('Topic Proportion by Year')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xticks(range(len(years)), years, rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 추가: 연도별 토픽 비중 데이터를 CSV 파일로 저장\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_file_path = 'topic_proportions_' + now + '.csv'\n",
        "with open(csv_file_path, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Year'] + [f'Topic {i+1}' for i in range(num_topics)])  # 헤더 행 작성\n",
        "    for year in years:\n",
        "        writer.writerow([year] + topic_proportions[year])  # 연도별 토픽 비중 데이터 작성"
      ],
      "metadata": {
        "id": "MPFSbcTv5ZFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# 연도별 토픽 분포 개별 선그래프\n",
        "# 연도별 토픽 분포를 저장할 딕셔너리\n",
        "year_topic_dist = {}\n",
        "\n",
        "# 연도별로 반복\n",
        "for row_index, row in data.iterrows():\n",
        "    year = str(row['application'])[:4]\n",
        "    if year not in year_topic_dist:\n",
        "        year_topic_dist[year] = [0] * lda_model.num_topics\n",
        "\n",
        "    title = row['inventionTitle']\n",
        "    abstract = row['astrtCont']\n",
        "    doc = [token for token in tokenized_patents[row_index] if token in dictionary.token2id]\n",
        "    doc_dist = lda_model.get_document_topics(bow=dictionary.doc2bow(doc))\n",
        "    for topic_id, prob in doc_dist:\n",
        "        year_topic_dist[year][topic_id] += prob\n",
        "\n",
        "# 각 토픽별로 연도 변화 추이 시각화\n",
        "years = sorted(year_topic_dist.keys())\n",
        "num_topics = lda_model.num_topics\n",
        "colors = plt.cm.rainbow(np.linspace(0, 1, num_topics))  # 토픽 개수에 따른 색상 팔레트 생성\n",
        "\n",
        "fig, axes = plt.subplots(nrows=num_topics, ncols=1, figsize=(12, num_topics * 4), sharex=True)\n",
        "\n",
        "for topic_id in range(num_topics):\n",
        "    values = [year_topic_dist[year][topic_id] for year in years]\n",
        "\n",
        "    ax = axes[topic_id]\n",
        "    ax.plot(range(len(years)), values, marker='o', color=colors[topic_id], label=f\"Topic {topic_id+1}\")\n",
        "    ax.set_ylim(0, 1600)  # y축 범위 설정 (0부터 1800까지)\n",
        "\n",
        "    # x축에 연도 표시\n",
        "    ax.set_xticks(range(len(years)))\n",
        "    ax.set_xticklabels(years, rotation=45, ha='right', fontsize=12)\n",
        "\n",
        "    ax.set_xlabel('Year', fontsize=14)\n",
        "    ax.set_ylabel('Proportion', fontsize=14)\n",
        "    ax.set_title(f'Topic {topic_id+1} Evolution', fontsize=16)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PnG-qsAEnzuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# 연도별 토픽 분포 개별 선그래프\n",
        "# 연도별 토픽 분포를 저장할 딕셔너리\n",
        "year_topic_dist = {}\n",
        "\n",
        "# 연도별로 반복\n",
        "for row_index, row in data.iterrows():\n",
        "    year = str(row['application'])[:4]\n",
        "    if year not in year_topic_dist:\n",
        "        year_topic_dist[year] = [0] * lda_model.num_topics\n",
        "\n",
        "    title = row['inventionTitle']\n",
        "    abstract = row['astrtCont']\n",
        "    doc = [token for token in tokenized_patents[row_index] if token in dictionary.token2id]\n",
        "    doc_dist = lda_model.get_document_topics(bow=dictionary.doc2bow(doc))\n",
        "    for topic_id, prob in doc_dist:\n",
        "        year_topic_dist[year][topic_id] += prob\n",
        "\n",
        "# 각 토픽별로 연도 변화 추이 시각화\n",
        "years = sorted(year_topic_dist.keys())\n",
        "num_topics = lda_model.num_topics\n",
        "colors = plt.cm.rainbow(np.linspace(0, 1, num_topics))  # 토픽 개수에 따른 색상 팔레트 생성\n",
        "\n",
        "fig, axes = plt.subplots(nrows=num_topics, ncols=1, figsize=(12, num_topics * 4), sharex=True)\n",
        "\n",
        "for topic_id in range(num_topics):\n",
        "    values = [year_topic_dist[year][topic_id] for year in years]\n",
        "\n",
        "    ax = axes[topic_id]\n",
        "    ax.plot(range(len(years)), values, marker='o', color=colors[topic_id], label=f\"Topic {topic_id+1}\")\n",
        "    ax.set_ylim(0, 800)  # y축 범위 설정 (0부터 1800까지)\n",
        "\n",
        "    # x축에 연도 표시\n",
        "    ax.set_xticks(range(len(years)))\n",
        "    ax.set_xticklabels(years, rotation=45, ha='right', fontsize=12)\n",
        "\n",
        "    ax.set_xlabel('Year', fontsize=14)\n",
        "    ax.set_ylabel('Proportion', fontsize=14)\n",
        "    ax.set_title(f'Topic {topic_id+1} Evolution', fontsize=16)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vYzrij-YyANH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "def get_word(word_id):\n",
        "    return dictionary.get(word_id)\n",
        "\n",
        "def get_topic_words(topic_id, top_n=10):\n",
        "    topic_words = lda_model.get_topic_terms(topic_id, topn=top_n)\n",
        "    return [get_word(word_id) for word_id, prob in topic_words]\n",
        "\n",
        "def get_topic_word_dist(lda_model, top_n=10):\n",
        "    num_topics = lda_model.num_topics\n",
        "    vocab_size = len(lda_model.id2word)\n",
        "    topic_word_dist = np.zeros((num_topics, vocab_size))\n",
        "    for topic_id in range(num_topics):\n",
        "        top_words = get_topic_words(topic_id, top_n)\n",
        "        for word in top_words:\n",
        "            word_id = lda_model.id2word.token2id.get(word, -1)\n",
        "            if word_id != -1:\n",
        "                topic_word_dist[topic_id, word_id] = 1\n",
        "    return topic_word_dist\n",
        "\n",
        "# 토픽-단어 분포 매트릭스 생성 (상위 10개 단어만 고려)\n",
        "topic_word_dist = get_topic_word_dist(lda_model, top_n=10)\n",
        "\n",
        "# k-means 클러스터링 수행\n",
        "num_clusters = 5  # 클러스터 수 설정\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "cluster_assignments = kmeans.fit_predict(topic_word_dist)\n",
        "\n",
        "# 클러스터별 토픽 출력\n",
        "for cluster_id in range(num_clusters):\n",
        "    cluster_topics = [i for i, c in enumerate(cluster_assignments) if c == cluster_id]\n",
        "    print(f'Cluster {cluster_id}:')\n",
        "    for topic_id in cluster_topics:\n",
        "        top_words = get_topic_words(topic_id, top_n=5)\n",
        "        print(f'  Topic {topic_id+1}: {\", \".join(top_words)}')\n",
        "    print()\n",
        "\n",
        "# 클러스터 정보를 딕셔너리로 변환\n",
        "cluster_dict = {}\n",
        "for cluster_id in range(num_clusters):\n",
        "    cluster_topics = [i for i, c in enumerate(cluster_assignments) if c == cluster_id]\n",
        "    cluster_dict[cluster_id] = cluster_topics\n",
        "\n",
        "# 네트워크 생성\n",
        "G = nx.Graph()\n",
        "\n",
        "# 노드 추가 (토픽 번호 = 노드 번호)\n",
        "for topic_id in range(lda_model.num_topics):\n",
        "    for cluster_id, topic_list in cluster_dict.items():\n",
        "        if topic_id in topic_list:\n",
        "            G.add_node(topic_id+1, label=f\"Topic {topic_id+1}\", cluster=cluster_id)\n",
        "            break\n",
        "\n",
        "# 엣지 추가 (토픽 간 유사도 기반)\n",
        "for i in range(lda_model.num_topics):\n",
        "    for j in range(i+1, lda_model.num_topics):\n",
        "        similarity = np.dot(topic_word_dist[i], topic_word_dist[j]) / (np.linalg.norm(topic_word_dist[i]) * np.linalg.norm(topic_word_dist[j]))\n",
        "        G.add_edge(i+1, j+1, weight=similarity)\n",
        "\n",
        "# 클러스터별 노드 색상 설정\n",
        "cluster_colors = {0: 'r', 1: 'g', 2: 'b', 3: 'c', 4: 'm'}\n",
        "node_colors = [cluster_colors[G.nodes[node]['cluster']] for node in G.nodes()]\n",
        "\n",
        "# 네트워크 시각화\n",
        "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "nx.draw(G, pos, with_labels=True, font_size=8, node_color=node_colors)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wk7aaX3knzwj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}